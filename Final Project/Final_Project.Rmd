---
title: "Project-One-Group-3"
author: "Jasmine Dogu, Christos Chen, Brian Wimmer - Group 3"
date: "01/11/2021"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: paper
  pdf_document:
    toc: yes
---
#Cleaning the Data 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, error=FALSE, cache = TRUE)
#Import packages 
library(tidyverse)
#library(reticulate)
library(tidytext)
library(textdata)
#use_python("/home/rstudio-user/.local/share/r-miniconda/envs/r-reticulate/bin/python",required= TRUE)
#install.packages("tidyverse")
library(tidyselect)
library(stringr)
library(caret)
library(e1071)
library(rpart)
library(RTextTools)
library(tm)
library(DMwR)
library(DT)
#install.packages("topicmodels")
library(tm)
library(plotly)
#install.packages("LDAvis")
library(LDAvis)
library(qdap)
library(topicmodels)
library(ggplot2)
library(googleLanguageR)
library(cld2)
library(wordnet)
library(qdapDictionaries)
library(datasets)
library(dplyr)
#install.packages("reticulate")
set.seed(03092000)
```

#About the Dataset
We will be using the “Spam Text Message Classification” dataset from Kaggle. It can be found [here](https://www.kaggle.com/team-ai/spam-text-message-classification).

The data consists of 5157 observations. Each observation is labeled “spam” or “ham”, with a base rate of 13% for spam. For reference, “ham” is a normal and non-spam text message. Each observation includes the message containing text.

#Background Research
We also wanted to do some background research regarding spam text messages to get an idea of how big a problem it is. We were extremely surprised to find that it was even more prominent than we expected. We discovered that around 3% of all text messages are spam related and that nearly 43 million Americans lost $10.5 billion to cellular text/call spams (in 2018). We also found information regarding steps that wireless providers utilize to minimize spam messages, including machine learning and artificial intelligence.


```{r}
#reading in the dataset 
df <- read.csv("/cloud/project/Datasets/Text_Message_Classification.csv")
head(df)
str(df)
head(df,5)
```


```{r}
library(wordnet)
library(textstem)
library(qdapDictionaries)
library(qdap)
library(iconv)

#CorpusList <-  iconv(, to ="utf-8")

dfCorpus = Corpus(VectorSource(df))  
#dfCorpus = as.character(dfCorpus)
inspect(dfCorpus) 

clean_corpus <- function(corpus) {
  # Remove punctuation
  corpus <- tm_map(corpus, removePunctuation(), lazy=TRUE)
  # Transform to lower case
  corpus <- tm_map(corpus, tolower(corpus, as.character()), lazy=TRUE)
  # Remove stopwords and strip whitespace
  corpus <- tm_map(corpus, removeWords(), words = stopwords("en"), stripWhitespace(), lazy=TRUE)
  # Lemmatize
  corpus <- tm_map(corpus, lemmatize_strings(corpus, as.vector()), lazy=TRUE)
  return(corpus)
}


?removePunctuation
?stripWhitespace
?lemmatize_strings
?tm_map
?content_transformer  #convert everything to the correct data type within the corpus

clean_corpus(dfCorpus)

```


```{r}
#Datasets for later 
df_spam <- filter(df, df$Category =="spam")
df_ham <-filter(df, df$Category =="ham")


#Removing punctuation
df2 = df %>% 
  select(Message) 

tm_map(df2, lemmatize_strings)

#%>%
# replace_contraction() 

df2$Message <- (str_replace_all(df2$Message, "[[:punct:]]", " "))


head(df2)

df_xx = df2 %>%
  unnest_tokens(word, "Message")  %>%

head(df_xx)

df_xx <- df_xx %>% 
  anti_join(stop_words)
  
df_xx <- filter(df_xx, !str_detect(word, "na")) #filtering out the na's in messages
df_xx
```

```{r}
#code prof wright gave to help us w lemmatize

#corpus[[1]]
#[1] " earnest roughshod document serves workable primer regions recent history make 
#terrific th-grade learning tool samuel beckett applied iranian voting process bard 
#black comedy willie loved another trumpet blast may new mexican cinema -bornin "
#> corpus <- tm_map(corpus, lemmatize_strings)
#> corpus[[1]]
#[1] "earnest roughshod document serve workable primer region recent history make 
#terrific th - grade learn tool samuel beckett apply iranian vote process bard black 
#comedy willie love another trumpet blast may new mexican cinema - bornin"
```


# SPAM Filtering
```{r}
#Removing punctuation
df2_spam = df_spam %>% 
  select(Message) 
#%>%
  #lemmatize_words(df$Message)

head(df2_spam)

df2_spam$Message <- (str_replace_all(df2_spam$Message, "[[:punct:]]", " "))
head(df2_spam)

df_xx_spam = df2_spam %>%
  unnest_tokens(word, "Message")
head(df_xx_spam)

df_xx_spam <- df_xx_spam %>% 
  anti_join(stop_words)

#%>%
 # replace_contraction()
  
df_xx_spam <- filter(df_xx_spam, !str_detect(word, "na")) #filtering out the na's in messages
df_xx_spam
```


# HAM Filtering 
```{r}
#Removing punctuation
df2_ham = df_ham %>% 
  select(Message) 
#%>%
  #lemmatize_words(df$Message)

head(df2_ham)

df2_ham$Message <- (str_replace_all(df2_ham$Message, "[[:punct:]]", " "))
head(df2_ham)

df_xx_ham = df2_ham %>%
  unnest_tokens(word, "Message")
head(df_xx_ham)

df_xx_ham <- df_xx_ham %>% 
  anti_join(stop_words)

  
df_xx_ham <- filter(df_xx_ham, !str_detect(word, "na")) #filtering out the na's in messages
df_xx_ham
```





# Joined Count
```{r}
head(df_xx %>%
  count(df_xx$word, sort = TRUE), 20)

df_xx_count <- df_xx %>%
  count(word, sort=TRUE)


df_xx_count$word <- as.factor(df_xx_count$word)
#df$Category <- ifelse(df$Category=='ham',0,1)

head(df_xx,5)
```

#Joined Sentiment 
```{r}
library(textdata)
get_sentiments('afinn') #generic table (word vs value)

df_sentiment_affin <- 
  df_xx %>%
  inner_join(get_sentiments("afinn")) # pull out only sentiment words

#View(df_sentiment_affin)
```

#SPAM Sentiment
```{r}
library(textdata)
get_sentiments('afinn') #generic table (word vs value)

df_sentiment_affin_spam <- 
  df_xx_spam %>%
  inner_join(get_sentiments("afinn")) # pull out only sentiment words

View(df_sentiment_affin_spam)
```

#HAM Sentiment
```{r}
library(textdata)
get_sentiments('afinn') #generic table (word vs value)

df_sentiment_affin_ham <- 
  df_xx_ham %>%
  inner_join(get_sentiments("afinn")) # pull out only sentiment words

View(df_sentiment_affin_ham)
```



#Joined Sentiment Chart
```{r}
table(df_sentiment_affin$value)


ggplot(data = df_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Spam/Ham Classification Sentiment Range")+
  theme_minimal()


```


#SPAM Sentiment Chart
```{r}
table(df_sentiment_affin_spam$value)


ggplot(data = df_sentiment_affin_spam, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Spam Text Sentiment Range")+
  theme_minimal()

```

#HAM Sentiment Chart
```{r}
table(df_sentiment_affin_ham$value)


ggplot(data = df_sentiment_affin_ham, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Ham Text Sentiment Range")+
  theme_minimal()
```


# Base Rate Analysis
```{r}
print(table(df$Category))
```

```{r}
print(prop.table(table(df$Category)))
```
At 13.41%, this is clearly a skewed data set, aka rare event.

# Base Rate Pie Chart
```{r, message = FALSE, warnings = FALSE}
#creating pie chart for base rate
count.data <- data.frame(
  class = c("Ham", "Spam"),
  n = c(4825, 747),
  prop = c(86.59, 13.41)
)

mycols <- c("#474d84","#8d84ac")

# Add label position
count.data <- count.data %>%
  arrange(desc(class)) %>%
  mutate(lab.ypos = cumsum(prop) - 0.5*prop)
count.data


br<- ggplot(count.data, aes(x = 2, y = prop, fill = class)) +
  geom_bar(stat = "identity", color = "white") +
  coord_polar(theta = "y", start = 0)+
  geom_text(aes(y = lab.ypos, label = prop), color = "white")+
  scale_fill_manual(values = mycols) +
  theme_void()+
  xlim(0.5, 2.5)
br
#ggsave(br,file="base_rate.png", bg = "transparent")
```

# Data Cleaning 

```{r}
#remove stop words, perform stemming/maybe lemmatization, handle aprosthropes, handle contractions, deal with numbers (?), spell correction if needed, language, SMOTE, account for text

```


```{r}

df_xx_count$word <- as.factor(df_xx_count$word) 
str(df_xx_count)

tokenword_plot = ggplotly(ggplot(
  data = head(df_xx_count, 30),
  aes(x = fct_reorder(word,n),
      y = n)
  ) + 
  geom_col() + 
  coord_flip()+
  theme_light()+
  xlab("Token Words")+
    ylab("Count") + ggtitle('Words that Appear Most in the Texts') )
tokenword_plot


df_xx <- tibble(df_xx)
```

#Creating Unigrams and Bigrams 
```{r}

#Bigrams 
df_xx_ngrams_2 <- df2 %>%
  unnest_tokens(word, Message, token = "ngrams", n=2)

head(datatable(df_xx_ngrams_2), 20)

df_xx_ngrams_2 %>% 
  count(word) %>% 
  arrange(desc(n))


#Trigrams
df_xx_ngrams_3 <- df2 %>%
  unnest_tokens(word, Message, token = "ngrams", n=3)

df_xx_ngrams_3 %>% 
  count(word) %>% 
  arrange(desc(n))

head(datatable(df_xx_ngrams_3), 20)
```


```{r}
id <- rownames(df)
df_tf <- cbind(id=id, df)
df_tf
#------ Here, df = data_math_tf in Prof Wright's code 


#Placing everything in one column and adding "other id" back in


#Removing punctuation and NAs
df_tf$Message <- (str_replace_all(df_tf$Message, "[[:punct:]]", " "))

df_tf$Message <- (str_remove_all(df_tf$Message, "NA")) 
df_tf

#Counting of words, with ID

word_count_id <- df_tf %>%
  unnest_tokens(word, Message) %>%
  count(id, word, sort = TRUE)

head(word_count_id, 8)

total_words_id <- word_count_id %>% 
  group_by(id) %>% 
  summarize(total = sum(n))

head(total_words_id, 12)

text_words_id <- left_join(word_count_id, total_words_id)

text_words_id <- text_words_id %>%
  bind_tf_idf(word, id, n)

datatable(text_words_id)

```

#Topic Modeling

## Beta
```{r}
#Remove stop words
word_count_id <- word_count_id %>% 
  anti_join(stop_words)

head(word_count_id, 15)

text_dtm <- word_count_id %>%
  cast_dtm(id, word, n)

text_lda <- LDA(text_dtm, k = 2, control = list(seed = 03092000))

text_topics <- tidy(text_lda, matrix = "beta")

text_documents <- tidy(text_lda, matrix = "gamma")

datatable(text_topics)

datatable(text_documents)

gammaVals = data.frame(text_documents)

gammaVals$document = as.numeric(gammaVals$document)
str(gammaVals)

gammaValsStats = gammaVals %>% 
  filter(document %in% sample(1:nrow(df), round(31, 0), replace = FALSE)) 
gammaValsStats

gammaValsStatsTopic1 = gammaValsStats %>% filter(topic == 1) 
gammaValsStatsTopic2 = gammaValsStats %>% filter(topic == 2) 
gammaValsStatsTopic2
```

```{r}
text_top_terms <- text_topics %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

datatable(text_top_terms)

plt <- text_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()

plt

```

```{r}
#Spread Plot 

beta_spread_text <- text_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1)) %>% 
  arrange(log_ratio)

#Two topics
datatable(beta_spread_text)

data_plt_text <- tail(beta_spread_text, 15)

data_plt_text <- rbind(data_plt_text, head(beta_spread_text, 15))

split_plt_text <- data_plt_text %>% 
  mutate(term = fct_reorder(term, log_ratio)) %>%
  ggplot(aes(x=term,
            y=log_ratio)
            )+ geom_col()+
            coord_flip() +
            ggtitle("The Top 15 Most Different Words Between the Two Topics")

split_plt_text
```



## Gamma 
```{r}
text_documents <- tidy(text_lda, matrix = "gamma")
datatable(text_documents)
```


# Topic Modeling - Latent Dirichlet Allocation
```{r}
install.packages('topicmodels')
library(topicmodels)

data("AssociatedPress")
AssociatedPress
```


```{r}
#Regular Expressions
df_regEx <- df
df_regEx['Free'] <- NA
df_regEx['Currency_Symbol'] <- NA
df_regEx['Digits'] <- NA
df_regEx['Link'] <- NA
df_regEx['Call'] <- NA
df_regEx['Mobile_Phone'] <- NA
df_regEx['XXX'] <- NA
df_regEx['Symbol'] <- NA
df_regEx['Please'] <- NA
df_regEx['Reply_Yes'] <- NA
df_regEx['FreeMsg'] <- NA
df_regEx['Won_Win'] <- NA
df_regEx['Eighteen'] <- NA

head(df_regEx)
```


```{r}
df_regEx$Currency_Symbol <- sapply(df_regEx$Message,str_detect, pattern = "\\$|£")
df_regEx$Free <- sapply(df_regEx$Message,str_detect, pattern = "([Ff][Rr][Ee][Ee])|([Uu][Rr][Gg][Ee][Nn][Tt])")
df_regEx$Digits <- sapply(df_regEx$Message,str_detect, pattern = "\\d{4,10}")
df_regEx$Link <- sapply(df_regEx$Message,str_detect, pattern = "([Ww]{3})|([Hh][Tt][Tt][Pp][Ss])|([Cc][Oo][Mm])")
df_regEx$Call <- sapply(df_regEx$Message,str_detect, pattern = "[Cc][Aa][Ll][Ll]")
df_regEx$Mobile_Phone <- sapply(df_regEx$Message,str_detect, pattern = "([Mm][Oo][Bb][Ii][Ll][Ee])|([Pp][Hh][Oo][Nn][Ee])")
df_regEx$XXX <- sapply(df_regEx$Message,str_detect, pattern = "[Xx]{3}")
df_regEx$Symbol <- sapply(df_regEx$Message,str_detect, pattern = "\\?|\\!")
df_regEx$Please <- sapply(df_regEx$Message,str_detect, pattern = "[Pp][Ll][Ee][Aa][Ss][Ee]")
df_regEx$Reply_Yes <- sapply(df_regEx$Message,str_detect, pattern = "([Yy][Ee][Ss])|([Nn][Oo])|([Rr][Ee][Pp][Ll][Yy]\\s[Yy][Ee][Ss])|([Rr][Ee][Pp][Ll][Yy])")
df_regEx$FreeMsg <- sapply(df_regEx$Message,str_detect, pattern = "[Ff][Rr][Ee][Ee][Mm][Ss][Gg]")
df_regEx$Won_Win <- sapply(df_regEx$Message,str_detect, pattern = "[Ww][OoIi][Nn]")
df_regEx$Eighteen <- sapply(df_regEx$Message,str_detect, pattern = "18+?")
regEx_removed <- df_regEx %>%
  select(-c("Category","Message"))
regEx_removed<- ifelse(regEx_removed==FALSE,0,1)
df_regEx<- cbind(df_regEx$Category, regEx_removed)
colnames(df_regEx)[1] <- "Category"
df_regEx<- as.data.frame(df_regEx)
df_regEx$Category <- ifelse(df_regEx$Category=='ham',0,1)  #if ham, 0, if spam 1
df_regEx <- df_regEx %>% mutate_if(is.character,as.factor)
df_regEx$Category <- as.factor(df_regEx$Category)
str(df_regEx)
View(df_regEx)
```

#SMOTE using KNN
```{r}
set.seed(03092000)

unique(df_regEx$Category)
result <- df_regEx %>% group_by(Category) %>% count()
result

Over = ((0.6 * 4825) - 747 ) / 747
Under = (0.4 * 4825) / (747 * Over)
Over_Perc = round(Over, 1) * 100
Over_Perc
Under_Perc = round(Under, 1) * 100
Under_Perc

str(df_regEx)
newData <- SMOTE(Category ~ ., df_regEx, perc.over = Over_Perc,perc.under=Under_Perc)
as.data.frame(table(newData$Category))

View(newData)
```

```{r}
library(reticulate)
import (pandas, as="pd")
reticulate::use_python("/home/rstudio-user/.local/share/r-miniconda/envs/r-reticulate/bin/python")
df_python = py_run_string(pd.read_csv("/cloud/project/Datasets/regEx.csv"))

```


```{r}


#library(reticulate)
#use_python("/home/rstudio-user/.local/share/r-miniconda/envs/r-reticulate/bin/python", required= TRUE)
#py_run_string()


#Sys.which("python")




#py_discover_config()
#py_install("pandas")
#py_install("scikit-learn")
#py_install("collections")
#py_install("GLIBCXX_3.4.22")
```



```{python}
#df_python = pd.read_csv("/cloud/project/Datasets/regEx.csv")
#import pandas as pd
#from collections import Counter
#from sklearn.datasets import make_classification
#from imblearn.over_sampling import SVMSMOTE 

#Y = df_python["Category"]
#X = df_python.drop(['Category'], axis=1)
#sm = SVMSMOTE(random_state=3092000)
#X_res, Y_res = sm.fit_resample(X, Y)

#df_svm_smote = pd.concat([Y_res,X_res], axis=1)

#df_svm_smote.to_csv('/cloud/project/Datasets/df_svm_smote2.csv', index=False) 
#GLIBCXX_3.4.22
#df_svm_smote.head()
```


SMOTE Dataset Using Python
```{r}
#utilized the dataframe we saved over from the jupyter notebook
df_smote_svm <- read.csv("/cloud/project/Datasets/df_svm_smote.csv")
df_smote_svm <- df_smote_svm %>% select(-"X") 
df_smote_svm[sapply(df_smote_svm, is.numeric)] <- lapply(df_smote_svm[sapply(df_smote_svm, is.numeric)],as.factor)
head(df_smote_svm)
```

```{r}
# Creating the Training and Testing Sets
set.seed(03092000)

data_train <- sample(1:nrow(df_regEx),
               round(0.7 * nrow(df_regEx), 0), 
               replace = FALSE)
data_test <- sample(1:nrow(df_regEx),
               round(0.3 * nrow(df_regEx), 0), 
               replace = FALSE)

data_train_smote <- sample(1:nrow(newData),
               round(0.7 * nrow(newData), 0), 
               replace = FALSE)
data_test_smote <- sample(1:nrow(newData),
               round(0.3 * nrow(newData), 0), 
               replace = FALSE)

data_train_smote_svm <- sample(1:nrow(df_smote_svm),
               round(0.7 * nrow(df_smote_svm), 0), 
               replace = FALSE)

data_test_smote_svm <- sample(1:nrow(df_smote_svm),
               round(0.3 * nrow(df_smote_svm), 0), 
               replace = FALSE)


#Assigning random selection to original df
original_train <- df_regEx[data_train, ] #Should contain 70% of data points
original_test <- df_regEx[data_test, ]

#Assigning random selection to smote df (this is for the model utilizing knn)
smote_train <- newData[data_train_smote, ] #Should contain 70% of data points
smote_test <- newData[data_test_smote, ]

#Assigning random selection to smote df (this is for the model utilizing svm)
smote_train_svm <- df_smote_svm[data_train_smote_svm, ] #Should contain 70% of data points
smote_test_svm <- df_smote_svm[data_test_smote_svm, ]

```

## Radial Kernel Training 
```{r}
#install.packages('e1071') 
library(e1071) 
set.seed(03092000)
classifier_smote <- svm(formula = Category ~ ., 
                 data = smote_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial')  #The kernel used in training and predicting

classifier_normal <- svm(formula = Category ~ ., 
                 data = original_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial')  #The kernel used in training and predicting
#based on a study that was done 

classifier_smote_svm <- svm(formula = Category ~ ., 
                 data = smote_train_svm, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial')  #The kernel used in training and predicting
#based on a study that was done 

```


```{r}
#Original
# Predicting the test set results 
set.seed(03092000)

y_pred_original <- predict(classifier_normal, newdata = original_test[-1]) 

#SMOTE
y_pred_original_smote <- predict(classifier_smote, newdata = smote_test[-1]) 

#SMOTE SVM
y_pred_original_smote_svm <- predict(classifier_smote_svm, newdata = smote_test_svm[-1]) 
```

### Confusion Matrix Results
```{r}
#Original
# Making a Confusion Matrix 
#install.packages("caret")
set.seed(03092000)

library(caret)
cm_original <- confusionMatrix(original_test$Category,y_pred_original, positive = "1")
cm_original

#SMOTE KNN
cm_smote <- confusionMatrix(smote_test$Category,y_pred_original_smote, positive = "1")
cm_smote

#SMOTE SVM
cm_smote_svm <- confusionMatrix(smote_test_svm$Category,y_pred_original_smote_svm, positive = "1")
cm_smote_svm
```


## Hyperparameter Tuning Proccess 
```{r}
library(e1071)
set.seed(03092000)
#Original
obj_original <- tune(svm, Category~., data = original_test, 
            ranges = list(gamma = 2^(-1:1), 
                          cost = 2^(2:4)),
            tunecontrol = tune.control(sampling = "fix"))
#obj_original ##gamma 0.5, cost 4
summary(obj_original)

#SMOTE - KNN
obj_smote <- tune(svm, Category~., data = smote_test, 
            ranges = list(gamma = 2^(-1:1), 
                          cost = 2^(2:4)),
            tunecontrol = tune.control(sampling = "fix"))
#obj_smote ##gamma 1, cost 4
summary(obj_smote)

#SMOTE - SVM
#gamma 1, cost 4
obj_smote_svm <- tune(svm, Category~., data = smote_test_svm, 
            ranges = list(gamma = 2^(-1:1), 
                          cost = 2^(2:4)),
            tunecontrol = tune.control(sampling = "fix"))
summary(obj_smote_svm)
obj_smote_svm
```


# Tuned Models
```{r}
#install.packages('e1071') 
library(e1071) 
set.seed(03092000)

classifier_smote_tuned <- svm(formula = Category ~ ., 
                 data = smote_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial',
                 gamma=1,
                 cost =4)  #The kernel used in training and predicting

classifier_normal_tuned <- svm(formula = Category ~ ., 
                 data = original_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial',
                 gamma =.5,
                 cost = 4)  #The kernel used in training and predicting
#based on a study that was done 
classifier_smote_tuned_svm <- svm(formula = Category ~ ., 
                 data = smote_train_svm, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial',
                 gamma =1,
                 cost = 4)  #The kernel used in training and predicting
#based on a study that was done 
```

# Tuned Prediction
```{r}
set.seed(03092000)
#Original
# Predicting the test set results 
y_pred_original_tuned <- predict(classifier_normal_tuned, newdata = original_test[-1]) 

#SMOTE KNN
y_pred_original_smote_tuned <- predict(classifier_smote_tuned, newdata = smote_test[-1]) 

#SMOTE SVM
y_pred_smote_tuned_svm <- predict(classifier_smote_tuned_svm, newdata = smote_test_svm[-1]) 

```

### Confusion Matrix Results for Tuned Models
```{r}
#Original
set.seed(03092000)
# Making a Confusion Matrix 
#install.packages("caret")
library(caret)
cm_original_tuned <- confusionMatrix(original_test$Category,y_pred_original_tuned, positive = "1")
cm_original_tuned

#SMOTE KNN
cm_smote_tuned <- confusionMatrix(smote_test$Category,y_pred_original_smote_tuned, positive = "1")
cm_smote_tuned

#SMOTE SVM
cm_smote_tuned_svm <- confusionMatrix(smote_test_svm$Category,y_pred_smote_tuned_svm, positive = "1")
cm_smote_tuned_svm
```





