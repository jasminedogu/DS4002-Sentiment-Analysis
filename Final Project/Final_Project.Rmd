---
title: "Project-One-Group-3"
author: "Jasmine Dogu, Christos Chen, Brian Wimmer - Group 3"
date: "01/11/2021"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: paper
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, error=FALSE, cache = TRUE)
#Import packages 
library(tidyverse)
#library(reticulate)
#install.packages("tidytext")
library(tidytext)
library(textdata)
#use_python("/home/rstudio-user/.local/share/r-miniconda/envs/r-reticulate/bin/python",required= TRUE)
#install.packages("tidyverse")
library(tidyselect)
library(stringr)
library(caret)
library(e1071)
library(rpart)
library(RTextTools)
library(tm)
library(DMwR)
#install.packages("DT")
library(DT)
#install.packages("topicmodels")
library(tm)
#install.packages("plotly")
library(plotly)
#install.packages("LDAvis")
library(LDAvis)
library(qdap)
#install.packages("topicmodels")
library(topicmodels)
library(ggplot2)
library(googleLanguageR)
library(cld2)
library(wordnet)
library(qdapDictionaries)
library(datasets)
library(dplyr)
#install.packages("reticulate")
set.seed(03092000)
```

#About the Dataset
We will be using the “Spam Text Message Classification” dataset from Kaggle. It can be found [here](https://www.kaggle.com/team-ai/spam-text-message-classification).

The data consists of 5157 observations. Each observation is labeled “spam” or “ham”, with a base rate of 13% for spam. For reference, “ham” is a normal and non-spam text message. Each observation includes the message containing text.

#Background Research
We also wanted to do some background research regarding spam text messages to get an idea of how big a problem it is. We were extremely surprised to find that it was even more prominent than we expected. We discovered that around 3% of all text messages are spam related and that nearly 43 million Americans lost $10.5 billion to cellular text/call spams (in 2018). We also found information regarding steps that wireless providers utilize to minimize spam messages, including machine learning and artificial intelligence.

#General Questions
1) Is there a Distinct Separation Between the Topics Found in Spam and Ham Text Messages?

- Topic Modeling - Latent Dirichlet Allocation using Gamma and Beta
- Classification - feature engineering using regular expressions

2) Can we Predict whether a Text will be Considered Spam or Ham?

- Synthetic Minority Oversampling Technique (SMOTE)
- Radial Support Vector Machines

#Hypotheses
Primary Hypothesis: 
Null - The variation between spam and non-spam messages within the LDA Topic Model Gamma will not be statistically significant (alpha of 0.05).
Alternative - The variation between spam and non-spam messages within the LDA Topic Model Gamma will be statistically significant (alpha of 0.05).

**In order to test our primary hypothesis, we will be conducting a two-sample t-test

Secondary Hypotheses:
Null - A SVM Kernel Model will classify spam messages with at a 0.9 recall rate or less. 
Alternative - A SVM Kernel Model will classify spam messages with a recall rate greater than 0.9.


```{r}
#reading in the dataset 
df <- read.csv("/cloud/project/Datasets/Text_Message_Classification.csv")
str(df)
head(df,5)
```

```{r}
#Datasets for later 
df_spam <- filter(df, df$Category =="spam")
df_ham <-filter(df, df$Category =="ham")

#Removing punctuation
df2 = df %>% 
  select(Message) 

#tm_map(df2, lemmatize_strings)

#%>%
# replace_contraction() 

df2$Message <- (str_replace_all(df2$Message, "[[:punct:]]", " "))


head(df2)

df_xx = df2 %>%
  unnest_tokens(word, "Message")

head(df_xx)

df_xx <- df_xx %>% 
  anti_join(stop_words)
  
df_xx <- filter(df_xx, !str_detect(word, "na")) #filtering out the na's in messages
df_xx
```

# SPAM Filtering
```{r}
#Removing punctuation
df2_spam = df_spam %>% 
  select(Message) 
#%>%
  #lemmatize_words(df$Message)

head(df2_spam)

df2_spam$Message <- (str_replace_all(df2_spam$Message, "[[:punct:]]", " "))
head(df2_spam)

df_xx_spam = df2_spam %>%
  unnest_tokens(word, "Message")
head(df_xx_spam)

df_xx_spam <- df_xx_spam %>% 
  anti_join(stop_words)

#%>%
 # replace_contraction()
  
df_xx_spam <- filter(df_xx_spam, !str_detect(word, "na")) #filtering out the na's in messages
df_xx_spam
```


# HAM Filtering 
```{r}
#Removing punctuation
df2_ham = df_ham %>% 
  select(Message) 
#%>%
  #lemmatize_words(df$Message)

head(df2_ham)

df2_ham$Message <- (str_replace_all(df2_ham$Message, "[[:punct:]]", " "))
head(df2_ham)

df_xx_ham = df2_ham %>%
  unnest_tokens(word, "Message")
head(df_xx_ham)

df_xx_ham <- df_xx_ham %>% 
  anti_join(stop_words)

  
df_xx_ham <- filter(df_xx_ham, !str_detect(word, "na")) #filtering out the na's in messages
df_xx_ham
```


```{r}
## Joined Count

head(df_xx %>%
  count(df_xx$word, sort = TRUE), 20)

df_xx_count <- df_xx %>%
  count(word, sort=TRUE)


df_xx_count$word <- as.factor(df_xx_count$word)
#df$Category <- ifelse(df$Category=='ham',0,1)

head(df_xx,5)
```

## Joined Sentiment 
```{r}
library(textdata)
get_sentiments('afinn') #generic table (word vs value)

df_sentiment_affin <- 
  df_xx %>%
  inner_join(get_sentiments("afinn")) # pull out only sentiment words

#View(df_sentiment_affin)
```

## SPAM Sentiment
```{r}
library(textdata)
get_sentiments('afinn') #generic table (word vs value)

df_sentiment_affin_spam <- 
  df_xx_spam %>%
  inner_join(get_sentiments("afinn")) # pull out only sentiment words

#View(df_sentiment_affin_spam)
```

## HAM Sentiment
```{r}
library(textdata)
get_sentiments('afinn') #generic table (word vs value)

df_sentiment_affin_ham <- 
  df_xx_ham %>%
  inner_join(get_sentiments("afinn")) # pull out only sentiment words

#View(df_sentiment_affin_ham)
```



## Joined Sentiment Chart
```{r}
table(df_sentiment_affin$value)


ggplot(data = df_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Spam/Ham Classification Sentiment Range")+
  theme_minimal()


```


## SPAM Sentiment Chart
```{r}
table(df_sentiment_affin_spam$value)


ggplot(data = df_sentiment_affin_spam, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Spam Text Sentiment Range")+
  theme_minimal()

```

## HAM Sentiment Chart
```{r}
table(df_sentiment_affin_ham$value)


ggplot(data = df_sentiment_affin_ham, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Ham Text Sentiment Range")+
  theme_minimal()
```


# Base Rate Assessment
```{r}
print(table(df$Category))
```

```{r}
print(prop.table(table(df$Category)))
```
At 13.41%, this is clearly a skewed data set, aka rare event.

# Base Rate Pie Chart
```{r, message = FALSE, warnings = FALSE}
#creating pie chart for base rate
count.data <- data.frame(
  class = c("Ham", "Spam"),
  n = c(4825, 747),
  prop = c(86.59, 13.41)
)

mycols <- c("#474d84","#8d84ac")

# Add label position
count.data <- count.data %>%
  arrange(desc(class)) %>%
  mutate(lab.ypos = cumsum(prop) - 0.5*prop)
count.data


br<- ggplot(count.data, aes(x = 2, y = prop, fill = class)) +
  geom_bar(stat = "identity", color = "white") +
  coord_polar(theta = "y", start = 0)+
  geom_text(aes(y = lab.ypos, label = prop), color = "white")+
  scale_fill_manual(values = mycols) +
  theme_void()+
  xlim(0.5, 2.5)
br
#ggsave(br,file="base_rate.png", bg = "transparent")
```

This chart shows the base rate again.


# Topic Modeling 

Topic Modeling is a form of unsupervised machine learning that works similar to clustering. Topic Modeling utilizes soft clustering, which means that documents can be composed of multiple documents for example. One of the most commonly utilized forms of topic modeling is Latent Dirichlet Allocation (LDA), which we will be utilizing in our topic modeling analysis. With our topic modeling analysis, we will aim to explore and answer our primary hypothesis - hopefully being able to easily identify and fit the two topics into spam and ham messages. 

```{r}
#remove stop words, perform stemming/maybe lemmatization, handle aprosthropes, handle contractions, deal with numbers (?), spell correction if needed, language, SMOTE, account for text

```

## Top Words 
```{r}

df_xx_count$word <- as.factor(df_xx_count$word) 
str(df_xx_count)

tokenword_plot = ggplotly(ggplot(
  data = head(df_xx_count, 30),
  aes(x = fct_reorder(word,n),
      y = n)
  ) + 
  geom_col() + 
  coord_flip()+
  theme_light()+
  xlab("Token Words")+
    ylab("Count") + ggtitle('Words that Appear Most in the Texts') )
tokenword_plot


df_xx <- tibble(df_xx)
```
The words that appear most in the texts above are illustrated above, with top 5 words being...
    - Call : 590 Appearances 
    - 2  : 533 Appearances 
    - ur  : 391 Appearances 
    - 4  :  327 Appearances 
    - gt  : 318 Appearances 


## Assessment of Unigrams and Bigrams 
```{r}

#Bigrams 
df_xx_ngrams_2 <- df2 %>%
  unnest_tokens(word, Message, token = "ngrams", n=2)

#head(datatable(df_xx_ngrams_2), 20)

df_xx_ngrams_2 %>% 
  count(word) %>% 
  arrange(desc(n))


#Trigrams
df_xx_ngrams_3 <- df2 %>%
  unnest_tokens(word, Message, token = "ngrams", n=3)

df_xx_ngrams_3 %>% 
  count(word) %>% 
  arrange(desc(n))

#head(datatable(df_xx_ngrams_3), 20)
```
  
We wanted to take a look at the bigrams and trigrams of the text data, in anticipation for potentially utilizing them later in our text classification model. 
  
The results of the bigrams demonstrated the the top 3 bigrams were "i m", "lt gt", and "i ll" with 425, 276, and 204 appearances respectively. This was informative as we realized that the frequency of the appearance of bigrams was not at a level that would make it particularly useful for feature engineering, given that the most frequent bigram, appeared less than 8% of the overall spam texts.
    
The resutls of the trigrams were equally as informative as they were unfruitful. The top 3 trigrams were "i don t", "i ll call", and "how are you" with 73, 57, and 44 appearances respectively. Once again, we realized that trigrams would not be a sufficient feature due to its infrequency within the overall dataset. 


```{r}
id <- rownames(df)
df_tf <- cbind(id=id, df)
df_tf
#------ Here, df = data_math_tf in Prof Wright's code 


#Placing everything in one column and adding "other id" back in


#Removing punctuation and NAs
df_tf$Message <- (str_replace_all(df_tf$Message, "[[:punct:]]", " "))

df_tf$Message <- (str_remove_all(df_tf$Message, "NA")) 
#df_tf

#Counting of words, with ID

word_count_id <- df_tf %>%
  unnest_tokens(word, Message) %>%
  count(id, word, sort = TRUE)

#head(word_count_id, 8)

total_words_id <- word_count_id %>% 
  group_by(id) %>% 
  summarize(total = sum(n))

#head(total_words_id, 12)

text_words_id <- left_join(word_count_id, total_words_id)

text_words_id <- text_words_id %>%
  bind_tf_idf(word, id, n)

datatable(text_words_id)

```

  We next wanted to create a table that contained aggregated information on the text messages, words, number of appearances, as well as tf, idf, and tf-idf metrics. Particularly, we were interested in the tf-idf metric for providing insights regarding the importance of words relative to a document or corpus. 

    - TFIDF, which stands for "term frequency–inverse document frequency", is a numerical statistic that will reflect how important a word is to a document in a corpus. 


## Beta and Gamma

  Within Latent Dirchlet Allocation, we want to particularly keep in mind two important metrics: beta and gamma. 
    - Gamma can be defined as the estimated proportion of words from a topic generated by that topic
    - Beta can be defined as the density of words within a topic
    
  
```{r}
#Remove stop words
word_count_id <- word_count_id %>% 
  anti_join(stop_words)

head(word_count_id, 15)

text_dtm <- word_count_id %>%
  cast_dtm(id, word, n)

text_lda <- LDA(text_dtm, k = 2, control = list(seed = 03092000))

text_topics <- tidy(text_lda, matrix = "beta")

text_documents <- tidy(text_lda, matrix = "gamma")

datatable(text_topics)

datatable(text_documents)

gammaVals = data.frame(text_documents)

gammaVals$document = as.numeric(gammaVals$document)
str(gammaVals)

set.seed(03092000)
gammaValsStats = gammaVals %>% 
  filter(document %in% sample(1:nrow(df), round(31, 0), replace = FALSE)) 
#gammaValsStats

gammaValsStatsTopic1 = gammaValsStats %>% filter(topic == 1) 
gammaValsStatsTopic2 = gammaValsStats %>% filter(topic == 2) 
#gammaValsStatsTopic2
```

The betas in our models were generally very low, ranging from .023 to 8.54 x 10^-9. This makes sense due to the nature of our text, being shorter SMS items rather than long, superfluous documents. As a general principle, the beta will increase as the number of words within the topic increases. 

  
We next examined the gamma values of the rates. In a very good model, we would expect our gamma rates to be able to generally classify documents with gamma values equivalent to the base rates of the data set ~13%/~87%. This is because we are hoping to equate the two unknown topics to the spam and non-spam observations. However, it is evident that our gamma values did not support that this topic modeling was effective method for the classification of our data set, as most most values remained around 0.50 for both topics 1 and two. 

## Primary Hypothesis Evaluation Two Sample T-Test

To properly assess the statistical significance of our gamma results between the topics, we chose to perform a two sample t-test. We first confirmed assumptions that would apply towards the use of a two sample t-test, such as independence, random sampling from a population, continuous data, normal distribution, and equal variance. 

```{r}
set.seed(03092000)
t.test(gammaValsStatsTopic1$gamma, gammaValsStatsTopic2$gamma)
```
  
Examining the t-test results, we found that we had a p-value of 0.9423. Because we are testing at the 0.05 significance level, we fail to reject the null hypothesis at the 0.05 level. Thus, we can conclude that the variation between spam and non-spam messages within the LDA Topic Model Gamma will not be statistically significant (alpha of 0.05). 

## Further Assessments for Differentiating Topics
```{r}
text_top_terms <- text_topics %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

datatable(text_top_terms)

plt <- text_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()

plt

```
  
Examining the two "unknown" topics, LDA topic modeling does not make it too obvious to qualitatively infer which topic is which, as shown when examining the top 15 words in terms of beta-valuation. Topic 1 showed slightly more structure, having top words that mentioned time, location, place. Topic 2 was slighly more conversational, with the utilization of slang appearing at times. A lot of words appeared to have many similarities. 

While this was not what we were ideally looking for, this was not too shocking as many spam messages purposefully mimic human linguistic patterns to trick humans into engaging with the message. 
  

 
```{r}
#Spread Plot 

beta_spread_text <- text_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1)) %>% 
  arrange(log_ratio)

#Two topics
datatable(beta_spread_text)

data_plt_text <- tail(beta_spread_text, 15)

data_plt_text <- rbind(data_plt_text, head(beta_spread_text, 15))

split_plt_text <- data_plt_text %>% 
  mutate(term = fct_reorder(term, log_ratio)) %>%
  ggplot(aes(x=term,
            y=log_ratio)
            )+ geom_col()+
            coord_flip() +
            ggtitle("The Top 15 Most Different Words Between the Two Topics")

split_plt_text
```

We then decided it could be useful to analyze the two topics by comparing the 15 most different words for each topic between the two topics. Ths was done using a log ratio of the beta values for each word. it appears that topic 1's most differing word was "house", while topic 2's most differing word was "week". it seems like there was a clear difference in the log ratio valuation separating the top 15 words for each topic, but this was not too evident and helpful in our classification process. 



#Regular Expressions: Feature Engineering

```{r}
#Regular Expressions
df_regEx <- df
df_regEx['Free'] <- NA
df_regEx['Currency_Symbol'] <- NA
df_regEx['Digits'] <- NA
df_regEx['Link'] <- NA
df_regEx['Call'] <- NA
df_regEx['Mobile_Phone'] <- NA
df_regEx['XXX'] <- NA
df_regEx['Symbol'] <- NA
df_regEx['Please'] <- NA
df_regEx['Reply_Yes'] <- NA
df_regEx['FreeMsg'] <- NA
df_regEx['Won_Win'] <- NA
df_regEx['Eighteen'] <- NA

head(df_regEx)
```


```{r}
df_regEx$Currency_Symbol <- sapply(df_regEx$Message,str_detect, pattern = "\\$|£")
df_regEx$Free <- sapply(df_regEx$Message,str_detect, pattern = "([Ff][Rr][Ee][Ee])|([Uu][Rr][Gg][Ee][Nn][Tt])")
df_regEx$Digits <- sapply(df_regEx$Message,str_detect, pattern = "\\d{4,10}")
df_regEx$Link <- sapply(df_regEx$Message,str_detect, pattern = "([Ww]{3})|([Hh][Tt][Tt][Pp][Ss])|([Cc][Oo][Mm])")
df_regEx$Call <- sapply(df_regEx$Message,str_detect, pattern = "[Cc][Aa][Ll][Ll]")
df_regEx$Mobile_Phone <- sapply(df_regEx$Message,str_detect, pattern = "([Mm][Oo][Bb][Ii][Ll][Ee])|([Pp][Hh][Oo][Nn][Ee])")
df_regEx$XXX <- sapply(df_regEx$Message,str_detect, pattern = "[Xx]{3}")
df_regEx$Symbol <- sapply(df_regEx$Message,str_detect, pattern = "\\?|\\!")
df_regEx$Please <- sapply(df_regEx$Message,str_detect, pattern = "[Pp][Ll][Ee][Aa][Ss][Ee]")
df_regEx$Reply_Yes <- sapply(df_regEx$Message,str_detect, pattern = "([Yy][Ee][Ss])|([Nn][Oo])|([Rr][Ee][Pp][Ll][Yy]\\s[Yy][Ee][Ss])|([Rr][Ee][Pp][Ll][Yy])")
df_regEx$FreeMsg <- sapply(df_regEx$Message,str_detect, pattern = "[Ff][Rr][Ee][Ee][Mm][Ss][Gg]")
df_regEx$Won_Win <- sapply(df_regEx$Message,str_detect, pattern = "[Ww][OoIi][Nn]")
df_regEx$Eighteen <- sapply(df_regEx$Message,str_detect, pattern = "18+?")
regEx_removed <- df_regEx %>%
  select(-c("Category","Message"))
regEx_removed<- ifelse(regEx_removed==FALSE,0,1)
df_regEx<- cbind(df_regEx$Category, regEx_removed)
colnames(df_regEx)[1] <- "Category"
df_regEx<- as.data.frame(df_regEx)
df_regEx$Category <- ifelse(df_regEx$Category=='ham',0,1)  #if ham, 0, if spam 1
df_regEx <- df_regEx %>% mutate_if(is.character,as.factor)
df_regEx$Category <- as.factor(df_regEx$Category)
str(df_regEx)
#View(df_regEx)
```

In order to conduct feature engineering for the data, we utilized Regular Expressions (or RegEx). We began by looking through the “spam” messages and searching for commonalities in words, symbols, phrases, numbers, etc. We made a list of these commonalities and then created regular expressions in order to isolate these features. We added a column for each of the 13 features that we decided on, and each observation indicated “T” or “F”. We then converted these to binary for modeling purposes.

Features:
Symbol - ? or !  
Reply_Yes - “Yes”, “No”, “reply yes”, “reply no”, “reply"  
Call - “call” 
Digits - 4-10 digit numbers/codes  
Link - “www”, “https”, and “com”  
FreeMsg - “freemsg”  
Free - “free” or “urgent”  
Won_Win - “won” or “win”  
Mobile_Phone - “mobile phone”  
Currency_Symbol - $ or £  
Please - “please”  
Eighteen - “18+”  
XXX - “xxx”. 

# Data Sampling
SMOTE is an oversampling technique for imbalanced data classification problems at the pre-processing step. It generates synthetic instances using the linear interpolation within the minority class and forces the region of the minority class to become broader. This is critical for our dataset given our base rate is 13%, indicating the presence of a rare event. There are various versions of SMOTE; for our purposes, the KNN and SVM variations are observed. 


## SMOTE-KNN Variation
```{r}
set.seed(03092000)

unique(df_regEx$Category)
result <- df_regEx %>% group_by(Category) %>% count()
result

Over = ((0.6 * 4825) - 747 ) / 747
Under = (0.4 * 4825) / (747 * Over)
Over_Perc = round(Over, 1) * 100
Over_Perc
Under_Perc = round(Under, 1) * 100
Under_Perc

str(df_regEx)
newData <- SMOTE(Category ~ ., df_regEx, perc.over = Over_Perc,perc.under=Under_Perc)
as.data.frame(table(newData$Category))

#View(newData)
```

Here, we utilize the KNN variant of the SMOTE algorithm. A few critical parameters for SMOTE-KNN is the perc.over and perc.under variables. The calculations for these parameters can be seen in the R chunk above. The output of the SMOTE-KNN algorithm will later be used when creating the testing and training splits. Next, we proceed to continuing our data sampling utilizing the SMOTE-SVM variation.

## SMOTE-SVM Variation (Using Python)
```{r}
#utilized the dataframe we saved over from the jupyter notebook
df_smote_svm <- read.csv("/cloud/project/Datasets/df_svm_smote.csv")
df_smote_svm <- df_smote_svm %>% select(-"X") 
df_smote_svm[sapply(df_smote_svm, is.numeric)] <- lapply(df_smote_svm[sapply(df_smote_svm, is.numeric)],as.factor)
head(df_smote_svm)
```

Having done the SMOTE-KNN data sampling, we can move onto creating the SMOTE-SVM data sampling. Although it is very similar to the SMOTE-KNN variant in the sense that it allows for an imbalanced dataset to become more balanced, it does have clear differences in the approach it takes to achieve this. SMOTE-SVM uses the SVM algorithm to locate the decision boundary defined by the support vectors and creates a new instance randomly along the lines, joining each minority class support vector. 

The R documentation for SMOTE-SVM was difficult to find and utilize. Although our group was able to find multiple research studies that conducted SMOTE-SVM and identified that it performed better for binary text classification, we were unable to find resources that helped guide us with using SMOTE-SVM in R. Therefore, we used Python for this portion of the project. The ipynb file showing the work can be found in the Final Projects directory of this project. In the above chunk, we are showing the data being read into our R STudio environment and a few manipulations to clean up the dataframe. After getting the dataframe from the SMOTE-SVM, we can move onto creating the training and testing groups and then onto creating the predictive models.

# Creating Predictive Models

## Creating Training and Testing Sets
When creating training and testing sets, it is important to note that this will be done three separate times: for the original imbalanced dataset, the SMOTE-KNN dataset, and the SMOTE-SVM dataset. Our team utilized a 70-30 split ratio. The work for this can be seen below.

```{r}
# Creating the Training and Testing Sets
set.seed(03092000)

#Original imbalanced dataset
data_train <- sample(1:nrow(df_regEx),
               round(0.7 * nrow(df_regEx), 0), 
               replace = FALSE)
data_test <- sample(1:nrow(df_regEx),
               round(0.3 * nrow(df_regEx), 0), 
               replace = FALSE)

#SMOTE-KNN variation
data_train_smote <- sample(1:nrow(newData),
               round(0.7 * nrow(newData), 0), 
               replace = FALSE)
data_test_smote <- sample(1:nrow(newData),
               round(0.3 * nrow(newData), 0), 
               replace = FALSE)

#SMOTE-SVM variation
data_train_smote_svm <- sample(1:nrow(df_smote_svm),
               round(0.7 * nrow(df_smote_svm), 0), 
               replace = FALSE)
data_test_smote_svm <- sample(1:nrow(df_smote_svm),
               round(0.3 * nrow(df_smote_svm), 0), 
               replace = FALSE)

#Assigning random selection to original df
original_train <- df_regEx[data_train, ] #Should contain 70% of data points
original_test <- df_regEx[data_test, ]

#Assigning random selection to smote df (this is for the model utilizing knn)
smote_train <- newData[data_train_smote, ] #Should contain 70% of data points
smote_test <- newData[data_test_smote, ]

#Assigning random selection to smote df (this is for the model utilizing svm)
smote_train_svm <- df_smote_svm[data_train_smote_svm, ] #Should contain 70% of data points
smote_test_svm <- df_smote_svm[data_test_smote_svm, ]
```

After creating the training and testing sets, we can move onto the predictive model building.Moving onto the actual predictive model creation, our group decided to use Radial Support Vector Machines. We believed that this algorithm would be the most successful given the nature of our dataset. Similarly, we found multiple research studies that supported this thought process. For example, in the International Journal of Scientific and Technology Research, there was an article that looked at classifying SPAM Messages with a dataset very similar to ours structurally. They looked at how their predictive models performed when using various algorithms, like KNN, K means, SVM, Linear Regression, Decision Trees to name a few. In their studies, SVM with a radial kernel performed the best overall with most metrics at or above 0.98, therefore we proceeded with selecting the Radial SVM model to implement on our data.

## SVM Radial Kernel Baseline Model Creation 
```{r}
#install.packages('e1071') 
library(e1071) 
set.seed(03092000)

#SMOTE-KNN Variation
classifier_smote <- svm(formula = Category ~ ., 
                 data = smote_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial')  #The kernel used in training and predicting
#Original imbalanced dataset
classifier_normal <- svm(formula = Category ~ ., 
                 data = original_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial')  #The kernel used in training and predicting
#based on a study that was done 

#SMOTE-SVM Variation
classifier_smote_svm <- svm(formula = Category ~ ., 
                 data = smote_train_svm, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial')  #The kernel used in training and predicting
#based on a study that was done 
```

Here, we are creating the three baseline models. These models will go through predictive testing and hyperparameter tuning in the next steps.

## Predicting Test Set Results 
```{r}
# Predicting the test set results 
set.seed(03092000)

#Original imbalanced data
y_pred_original <- predict(classifier_normal, newdata = original_test[-1]) 

#SMOTE-KNN variation
y_pred_original_smote <- predict(classifier_smote, newdata = smote_test[-1]) 

#SMOTE-SVM variation
y_pred_original_smote_svm <- predict(classifier_smote_svm, newdata = smote_test_svm[-1]) 
```

## Confusion Matrix Results
```{r}
# Making a Confusion Matrix 
set.seed(03092000)
#install.packages("caret")
library(caret)

#Original imbalanced Data
cm_original <- confusionMatrix(original_test$Category,y_pred_original, positive = "1")
cm_original

#SMOTE-KNN variation
cm_smote <- confusionMatrix(smote_test$Category,y_pred_original_smote, positive = "1")
cm_smote

#SMOTE-SVM variation
cm_smote_svm <- confusionMatrix(smote_test_svm$Category,y_pred_original_smote_svm, positive = "1")
cm_smote_svm
```

Looking at the confusion matrices, we can see that our models are performing fairly well prior to the hyperparameter tuning. The model built on the original imbalanced data seems to have the best performance across the board when looking at the different metrics. However, compared to the SMOTE-KNN and SMOTE-SVM variants, we can see that currently the SVM model built using the SMOTE-KNN data sampling is performing better. We move onto hyperparameter tuning.

## Hyperparameter Tuning Proccess 
```{r}
library(e1071)
set.seed(03092000)
#Original imbalanced dataset
obj_original <- tune(svm, Category~., data = original_test, 
            ranges = list(gamma = 2^(-1:1), 
                          cost = 2^(2:4)),
            tunecontrol = tune.control(sampling = "fix"))
#obj_original ##gamma 0.5, cost 4
summary(obj_original)

#SMOTE - KNN variation
obj_smote <- tune(svm, Category~., data = smote_test, 
            ranges = list(gamma = 2^(-1:1), 
                          cost = 2^(2:4)),
            tunecontrol = tune.control(sampling = "fix"))
#obj_smote ##gamma 1, cost 4
summary(obj_smote)

#SMOTE - SVM variation
obj_smote_svm <- tune(svm, Category~., data = smote_test_svm, 
            ranges = list(gamma = 2^(-1:1), 
                          cost = 2^(2:4)),
            tunecontrol = tune.control(sampling = "fix"))
#gamma 1, cost 4
summary(obj_smote_svm)
```

Using our tuning function, we find the optimal gamma and cost values. The tuning function tests a range of gamma and cost values, and determines the most efficient and optimized by looking at the error rates. By utilizing the optimized hyperparameters, we hope to increase the performance of our models. We move onto building the tuned models.

##SVM Radial Kernal Tuned Model Creation
```{r}
#install.packages('e1071') 
library(e1071) 
set.seed(03092000)
#SMOTE-KNN Variation  
classifier_smote_tuned <- svm(formula = Category ~ ., 
                 data = smote_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial',
                 gamma=1,
                 cost =4)  #The kernel used in training and predicting
#Original imbalanced data
classifier_normal_tuned <- svm(formula = Category ~ ., 
                 data = original_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial',
                 gamma =.5,
                 cost = 4)  #The kernel used in training and predicting
#based on a study that was done 

#SMOTE-SVM Variation
classifier_smote_tuned_svm <- svm(formula = Category ~ ., 
                 data = smote_train_svm, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial',
                 gamma =1,
                 cost = 4)  #The kernel used in training and predicting
#based on a study that was done 
```

After figuring out the optimized gamma and cost values, we re-run our models in hopes of creating more efficient and accurate models. 

## Predicting Test Set Results - Tuned Models
```{r}
set.seed(03092000)
#Original imbalanced data
# Predicting the test set results 
y_pred_original_tuned <- predict(classifier_normal_tuned, newdata = original_test[-1]) 

#SMOTE-KNN variation
y_pred_original_smote_tuned <- predict(classifier_smote_tuned, newdata = smote_test[-1]) 

#SMOTE-SVM variation
y_pred_smote_tuned_svm <- predict(classifier_smote_tuned_svm, newdata = smote_test_svm[-1]) 
```

### Confusion Matrix Results for Tuned Models
```{r}
set.seed(03092000)
# Making a Confusion Matrix 
#install.packages("caret")
library(caret)

#Original imabalanced data
cm_original_tuned <- confusionMatrix(original_test$Category,y_pred_original_tuned, positive = "1")
cm_original_tuned

#SMOTE-KNN Variation
cm_smote_tuned <- confusionMatrix(smote_test$Category,y_pred_original_smote_tuned, positive = "1")
cm_smote_tuned

#SMOTE-SVM Variation
cm_smote_tuned_svm <- confusionMatrix(smote_test_svm$Category,y_pred_smote_tuned_svm, positive = "1")
cm_smote_tuned_svm
```

Here, we have the confusion matrices for all three tuned models. We can see that similar to the un-tuned, baseline models, the model built using the original imbalanced data performed the best. When looking at the performance of the tuned models using the SMOTE data sampling, it can be seen that the SVM variation of SMOTE performed significantly better than the KNN variation of the SMOTE algorithm. 

Although overall, the models built on the original, imbalanced dataset performed the best, these are not the optimized models. Most machine learning algorithms for classification predictive models are designed and demonstrated on problems that assume an equal distribution of the data. As a result, this does mean that an application of a model may focus on learning the characteristics of the abundant observations only, neglecting the examples from the minority class. For this reason, it's important to utilize an oversampling technique like SMOTE. Therefore, we selected the tuned SMOTE-KNN model as our optimized, best-performing model. However, it is possible that because SMOTE was utilized, there may have been some overfitting to the data. This is critical to keep in mind, and should be explored in future studies. 

The confusion matrix showing all of the models and their performances can be found below. 

#Final Predictive Model Results
```{r}
#install.packages("kableExtra")
library(kableExtra)
Model = c("Untuned", "Tuned", "Untuned", "Tuned", "Untuned","Tuned")
Dataset = c("Original Data", "Original Data", "SMOTE-KNN", "SMOTE-KNN", "SMOTE-SVM", "SMOTE-SVM")
Kappa = c(0.8824,0.9172, 0.9169, 0.9371,0.8205, 0.8281)
Sensitivity = c(0.9947,0.9950,0.9768,0.9840, 0.9746, 0.9832)
Accuracy = c(0.9743,0.9815,0.9619,0.9712, 0.9102, 0.9140)
results = data.frame(Model, Dataset, Kappa, Sensitivity, Accuracy)
kable(results)
```

As a result of our findings, because our SVM Model utilizing the SMOTE-KNN data sampling algorithm had a recall rate of 98.40%, which is greater than the 90% recall rate as mentioned in our secondary null hypothesis, we reject the null hypothesis. Our results are statistically significant.

#Limitations
When looking at how our model and analysis could be implemented in the future, it is important to note some of the limitations. 

Geographical Boundaries - Our data was gathered from messages originating in the UK and Singapore. As text messages can have a high range of variation around the world (in syntax, morphology, slang, etc.), the usability of our model would have to be modified. This would be dependent on where the data was collected, and the cultural and language considerations that we would have to examine.

Generalizability - The utilization of SMOTE allowed us to replicate “spam” events in order to more accurately build our model. The SMOTE method increases the likelihood of overfitting within the model, which is something we would want to try to minimize.

Time Sensitive - As civilizations grow and change over time, so does the way in which we communicate. Spam messages, like other scams, vary in their common approaches overtime. This essentially means that they can be very dependent on common trends at the time. Our model does not take this into account, as the messages collected for the dataset were all from around the same time period.

SMOTE Documentation - When researching the implementation of SMOTE, we found there to be a lack of documentation of how the SMOTE-SVM algorithm works in R. We would hope for there to be more information and clarification released in the future.

#Future Analysis
There are a multitude of things that we would like to look at in the future, in order to better understand spam messages and how different models can evaluate these messages differently.

Other Types of Messages - There are various outlets for messaging, many of which we believe would be interesting to explore the similarities to spam text messaging. We believe that running similar modeling with email messages could be highly beneficial in discovering commonalities between the two messaging platforms. 

Different Locations - Looking at data solely from the United States could be both more relevant to our audience, and allow us to see similarities and differences between spam messaging techniques from around the world.

Utilize Other Features - We isolated the features for our model by looking at the spam messages and trying to find common numbers, phrases, words, and symbols. In a future analysis, we could evaluate the messages in a different manner and be able to include different (and possibly better) features within the model.

Sampling Methods - As we only utilized the Synthetic Minority Oversampling Technique (SMOTE) within our model, we would want to explore other strategies in the future in order to see a possible change in performance.

Cost Consideration - Although we looked at a cost matrix to determine possible cost savings for filtering out more spam messages for wireless customers, we could examine different biases to inject into the cost matrix. This would allow for maximum organization benefit, while more accurately portraying our cost savings numbers.

